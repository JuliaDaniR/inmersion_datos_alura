# -*- coding: utf-8 -*-
"""credito_banco_aleman_inmersion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SNIZ4R88hs9bXMQyqfUQyhzE9SGfFksH

# INMERSIÓN DE DATOS CON PYTHON
"""

# prompt: Importa los siguientes módulos con sus respectivos alias: pandas, matplotlib, seaborn, , drive de google colab, warning

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import warnings

drive.mount('/content/Drive')
warnings.filterwarnings('ignore')

pd.set_option('display.max_columns', None)
global df_banco, resultados

df_banco = pd.read_csv('/content/Drive/MyDrive/german_credit.csv')
df_banco.head()

df_banco.shape

df_banco.columns

df_banco.info()

df_banco.account_check_status.value_counts().index

columnas = list(df_banco.select_dtypes(include='object').columns)
for columna in columnas:
  print(f'El nombre de la columna: {columna}')
  print(list(df_banco[f'{columna}'].value_counts().index))
  print('\n')

# dic = {'yes': 1, 'no': 0}
# df_banco['foreign_worker'] = df_banco['foreign_worker'].map(dic)
# df_banco['foreign_worker']
# df_banco.foreign_worker.value_counts()

def procesar_datos():
  global df_banco
  df_banco = df_banco.drop_duplicates() if df_banco.duplicated().any() else df_banco
  df_banco = df_banco.dropna() if df_banco.isnull().values.any() else df_banco

  a = {'no checking account': 4,
      '>= 200 DM / salary assignments for at least 1 year': 3,
      '0 <= ... < 200 DM': 2,
      '< 0 DM': 1
  }
  df_banco['account_check_status'] = df_banco['account_check_status'].map(a)

  a = { 'no credits taken/ all credits paid back duly' : 1,
      'all credits at this bank paid back duly' : 2,
      'existing credits paid back duly till now' : 3,
      'delay in paying off in the past' : 4,
      'critical account/ other credits existing (not at this bank)' : 5
  }
  df_banco['credit_history'] = df_banco['credit_history'].map(a)

  a = {'car (new)' : 1,
      'car (used)' : 2,
      'furniture/equipment' : 3,
      'radio/television' : 4,
      'domestic appliances' : 5,
      'repairs' : 6,
      'education' : 7,
      '(vacation - does not exist?)' : 8,
      'retraining' : 9,
      'business' : 10,
      'others' : 11
  }
  df_banco['purpose'] = df_banco['purpose'].map(a)

  a = {'unknown/ no savings account' : 1,
      '.. >= 1000 DM ' : 2,
      '500 <= ... < 1000 DM ' : 3,
      '100 <= ... < 500 DM' : 4,
      '... < 100 DM' : 5
  }
  df_banco['savings'] = df_banco['savings'].map(a)

  a = {'.. >= 7 years' : 1,
      '4 <= ... < 7 years' : 2,
      '1 <= ... < 4 years' : 3,
      '... < 1 year ' : 4,
      'unemployed' : 5
  }
  df_banco['present_emp_since'] = df_banco['present_emp_since'].map(a)

  a = {'male : divorced/separated' : 1,
      'female : divorced/separated/married' : 2,
      'male : single' : 3,
      'male : married/widowed' : 4,
      'female : single' : 5
  }
  df_banco['personal_status_sex'] = df_banco['personal_status_sex'].map(a)

  a = {'none' : 1,
      'co-applicant' : 2,
      'guarantor' : 3
  }
  df_banco['other_debtors'] = df_banco['other_debtors'].map(a)

  a = {'real estate' : 1,
      'if not A121 : building society savings agreement/ life insurance' : 2,
      'if not A121/A122 : car or other, not in attribute 6' : 3,
      'unknown / no property' : 4
  }
  df_banco['property'] = df_banco['property'].map(a)

  a = {'bank' : 1,
      'stores' : 2,
      'none' : 3
  }
  df_banco['other_installment_plans'] = df_banco['other_installment_plans'].map(a)

  a = {'rent' : 1,
      'own' : 2,
      'for free' : 3
  }
  df_banco['housing'] = df_banco['housing'].map(a)

  a = {'unemployed/ unskilled - non-resident' : 1,
      'unskilled - resident' : 2,
      'skilled employee / official' : 3,
      'management/ self-employed/ highly qualified employee/ officer' : 4
  }
  df_banco['job'] = df_banco['job'].map(a)

  a = {'yes, registered under the customers name ' : 1,
      'none' : 0
  }
  df_banco['telephone'] = df_banco['telephone'].map(a)

  a = {'yes' : 1,
      'no' : 0
  }
  df_banco['foreign_worker'] = df_banco['foreign_worker'].map(a)

procesar_datos()
df_banco.sample(5)

variables_discretas = ['personal_status_sex', 'age',
                       'duration_in_month', 'credit_amount','default']
df_banco[variables_discretas].tail(3)

dic_sexo = {2:1,5:1,1:0,3:0,4:0}
df_banco['sexo'] = df_banco['personal_status_sex'].map(dic_sexo)
df_banco['sexo']

def feature_engineering():
  global df_banco
  dic_sexo = {2:1, 5:1, 1:0, 3:0, 4:0}
  dic_est_civil = {3:1, 5:1, 1:0, 2:0, 4:0}
  df_banco['sexo'] = df_banco['personal_status_sex'].map(dic_sexo)
  df_banco['estado_civil'] = df_banco['personal_status_sex'].map(dic_est_civil)
  df_banco['rango_edad'] = pd.cut(x = df_banco['age'],
                                  bins=[18, 30, 40, 50, 60, 70, 80],
                                  labels = [1, 2, 3, 4, 5, 6]).astype(int)
  df_banco['rango_plazos_credito']=pd.cut(x = df_banco['duration_in_month'],
                                            bins=[1, 12, 24, 36, 48, 60, 72],
                                            labels = [1, 2, 3, 4, 5, 6]).astype(int)
  df_banco['rango_valor_credito']=pd.cut(x = df_banco['credit_amount'],
                                           bins=[1, 1000, 2000, 3000, 4000,
                                                 5000, 6000, 7000, 8000, 9000,
                                                 10000, 11000, 12000, 13000,
                                                 14000, 15000, 16000, 17000,
                                                 18000, 19000, 20000],
                                           labels = [1, 2, 3, 4, 5, 6, 7, 8, 9,
                                                     10, 11, 12, 13, 14, 15, 16,
                                                     17, 18, 19, 20]).astype(int)
  df_banco = df_banco.drop(columns=['personal_status_sex','age',
                                    'duration_in_month','credit_amount'])

feature_engineering()
df_banco.head(5)

df_banco.describe()

# Configuración del estilo de Seaborn
sns.set(style="whitegrid")

# Crear la figura y los ejes
plt.figure(figsize=(5, 4))

# Generar el histograma usando Seaborn
sns.countplot(data=df_banco, x='sexo')

# Agregar títulos y etiquetas
plt.title("Distribución por Sexo", fontsize=16)
plt.xlabel("Sexo", fontsize=14)
plt.ylabel("Frecuencia", fontsize=14)

# Mostrar el gráfico
plt.show()

def analisis_exploratorio():
  global df_banco
  histogramas = ['sexo','estado_civil','rango_plazos_credito','rango_edad','default']
  lista_histogramas = list(enumerate(histogramas))
  plt.figure(figsize = (30,20))
  plt.title('Histogramas')
  for i in lista_histogramas:
    plt.subplot(3, 2, i[0]+1)
    sns.countplot(x = i[1], data = df_banco)
    plt.xlabel(i[1], fontsize=20)
    plt.ylabel('Total', fontsize=20)

analisis_exploratorio()



"""# Desafios
 1. Analizar los datos de las distribuciones e identificar si hay algun valor o registros que no se deben considerar para el modelo.
 2. Investiga que es y como crear un mapa de calor para analizar la correlación de las variables.
 3. Crear una conclusión para cada uno de los gráficos del histograma. Mirar los datos y extraer conclusiones, porque es una habilidad esencial.
"""

# Desafio 1
def feature_engineering_new():
  global df_banco
  dic_sexo = {2:1, 5:1, 1:0, 3:0, 4:0}
  dic_est_civil = {3:1, 5:1, 1:0, 2:0, 4:0}
  df_banco['sexo'] = df_banco['personal_status_sex'].map(dic_sexo)
  df_banco['estado_civil'] = df_banco['personal_status_sex'].map(dic_est_civil)
  df_banco['rango_edad'] = pd.cut(x = df_banco['age'],
                                  bins=[18, 30, 40, 65, 80],
                                  labels=[1, 2, 3, 4]).astype(int)
                                  #extendí cada rango
  df_banco['rango_plazos_credito']=pd.cut(x = df_banco['duration_in_month'],
                                            bins=[1, 12, 24, 36, 72],
                                            labels=[1, 2, 3, 4]).astype(int)
  df_banco['rango_valor_credito']=pd.cut(x = df_banco['credit_amount'],
                                           bins=[1, 1000, 2000, 3000, 4000,
                                                 5000, 6000, 7000, 8000, 9000,
                                                 10000, 11000, 12000, 13000,
                                                 14000, 15000, 16000, 17000,
                                                 18000, 19000, 20000],
                                           labels = [1, 2, 3, 4, 5, 6, 7, 8, 9,
                                                     10, 11, 12, 13, 14, 15, 16,
                                                     17, 18, 19, 20]).astype(int)
  df_banco = df_banco.drop(columns=['personal_status_sex','age',
                                    'duration_in_month','credit_amount'])

feature_engineering_new()
df_banco.head(5)

def analisis_exploratorio_new():
    global df_banco

    # Columnas para los histogramas
    histogramas = ['sexo', 'estado_civil', 'rango_plazos_credito', 'rango_edad', 'default']
    lista_histogramas = list(enumerate(histogramas))

    # Configurar estilo de fondo y contexto
    sns.set_style("dark")  # Opciones: "whitegrid", "darkgrid", "white", "dark", "ticks"
    plt.figure(figsize=(30, 20))
    plt.gcf().set_facecolor("black")  # Fondo del canvas

    # Colores específicos para los gráficos
    colores = ["#FF5733", "#33FF57", "#3357FF", "#F1C40F", "#9B59B6"]

    # Título general
    plt.suptitle('Histogramas', fontsize=48, color="white")

    for i in lista_histogramas:
        plt.subplot(3, 2, i[0] + 1)
        sns.countplot(x=i[1], data=df_banco, color=colores[i[0]])
        plt.xlabel(i[1], fontsize=24, color="white")
        plt.ylabel('Total', fontsize=24, color="white")
        plt.xticks(fontsize=22, color="white")
        plt.yticks(fontsize=22, color="white")

    # Ajustar diseño
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

analisis_exploratorio_new()

# Desafio 2
# Calcular la matriz de correlación
correlacion = df_banco.corr()

# Crear el mapa de calor de todas las variables
plt.figure(figsize=(20, 8))  # Tamaño del gráfico
sns.heatmap(correlacion, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Mapa de calor de correlación')
plt.show()

****

"""## Desafio 3 - Clase 1

### Conclusión histogramas:

1- Sexo: Indica que el 40% de las personas registradas son hombres y el resto mujeres.

2- Estado civil: Indica que hay mayor porcentaje de personas solteras que casadas o divorciadas.

3- Plazos créditos: Indica que el mayor porcentaje de créditos otorgados es entre 12 y 24 meses y el menor entre 36 y 72 meses.

4- Rango de edad: Indica que el mayor porcentaje de clientes está entre los 18 y 30 años y que hay muy poco porcentaje de personas mayores a 65 años

# 4. Construcción de Modelos
"""

# prompt: Importa las siguientes bibliotecas train_test_split, LogisticRegression, Decision TreeClassifier, RandomForestClassifier, GaussianNB, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler

# prompt: Por favor generame un ejemplo de modelo de machine learning  de clasificacion en phyton

def construir_modelos():
    global df_banco, resultados

    # Separar variables predictoras y variable objetivo
    X = df_banco.drop('default', axis=1)
    y = df_banco['default']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Definición de modelos
    modelos = {
        "Regresión Logística": LogisticRegression(),
        "Árbol de Decisión": DecisionTreeClassifier(),
        "Bosque Aleatorio": RandomForestClassifier(),
        "Naive Bayes": GaussianNB()
    }

    # Lista para almacenar resultados
    resultados_lista = []

    for nombre, modelo in modelos.items():
        # Entrenamiento y predicción
        modelo.fit(X_train, y_train)
        y_pred = modelo.predict(X_test)
        y_prob = modelo.predict_proba(X_test)[:, 1]  # Probabilidades para la clase positiva

        # Métricas de evaluación
        exactitud = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        sensibilidad = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_prob)

        # Agregar resultados a la lista
        resultados_lista.append({
            'Modelo': nombre,
            'Exactitud': exactitud,
            'Precisión': precision,
            'Sensibilidad': sensibilidad,
            'F1-score': f1,
            'AUC': auc
        })

    # Convertir la lista de resultados a un DataFrame
    resultados = pd.DataFrame(resultados_lista)
    print(resultados)

construir_modelos()

"""# 5. Selección del Modelo"""

def visualiza_resultados():
  results_df = resultados.copy()
  results_df.set_index('Modelo', inplace=True)

  # Transponer el DataFrame para facilitar la representación
  results_df = results_df.T
  colors = ['#0077b6', '#CDDBF3','#9370DB', '#DDA0DD']

  # Gráfico de barras agrupadas para cada métrica
  results_df.plot(kind='bar', figsize=(12, 6), colormap='viridis', rot=0, color=colors)
  plt.title('Comparación de Métricas por Modelo')
  plt.xlabel('Métricas')
  plt.ylabel('Puntuación')
  plt.legend(title='Modelos')
  plt.tight_layout()
  plt.show()

  # @title Texto de título predeterminado
  from IPython.display import HTML, display

  # Texto que quieres centrar
  texto = "¿Cuál de estos modelos seleccionarías y por qué?"

  # Crear una celda HTML con el texto centrado
  display(HTML(f"{texto}"))

visualiza_resultados()



"""# Desafio 1 "Clase 2"

Averiguar un poco sobre la matriz de confusión, de qué se trata, qué es una matriz de confusión, cómo se lee una matriz de confusión, qué tipos de errores puedes identificar.
A través de una matriz de confusión Y de qué manera según el problema de negocio que estés tratando cuál métrica debes priorizar porque en muchos casos Si no digo en la mayoría cuando mejoras la precisión desmejora la sensitividad Y si mejor la sensitividad que es el mismo recall entonces desmejorar la precisión entonces hay que tener muy claro Qué es y cuál es la variable que nosotros queremos tener en cuenta para ser más efectivos en nuestra clasificación

## Matriz de Confusión
La matriz de confusión es una herramienta fundamental en la evaluación de modelos de clasificación, especialmente para entender el desempeño del modelo más allá de métricas generales como la exactitud. Es una tabla que permite comparar las predicciones del modelo con los valores reales de las clases objetivo, proporcionando una visión detallada de los aciertos y errores del modelo.

## Cómo se lee una matriz de confusión
Una matriz de confusión para un problema binario (con clases Positiva y Negativa) tiene la siguiente estructura:

## Predicción Positiva	Predicción Negativa
| **Clase Real**       | **Predicción Positiva**          | **Predicción Negativa**          |
|-----------------------|----------------------------------|-----------------------------------|
| **Clase Positiva**    | Verdaderos Positivos (TP)       | Falsos Negativos (FN)            |
| **Clase Negativa**    | Falsos Positivos (FP)           | Verdaderos Negativos (TN)        |

- **TP (True Positives):** Casos en los que el modelo predijo correctamente la clase positiva.
- **FN (False Negatives):** Casos en los que el modelo predijo negativo, pero eran positivos en realidad.
- **FP (False Positives):** Casos en los que el modelo predijo positivo, pero eran negativos en realidad.
- **TN (True Negatives):** Casos en los que el modelo predijo correctamente la clase negativa.

## Tipos de errores identificables
La matriz de confusión ayuda a identificar dos tipos principales de errores:

1. Falsos Negativos (FN): Casos relevantes que el modelo no detecta.

- Ejemplo: Un modelo no detecta que un paciente tiene una enfermedad grave.
- Impacto: Puede ser crítico en problemas médicos o de seguridad.

2. Falsos Positivos (FP): Casos irrelevantes que el modelo clasifica incorrectamente como relevantes.

- Ejemplo: Un modelo clasifica como fraudulenta una transacción legítima.
- Impacto: Puede generar costos innecesarios o pérdida de confianza del usuario.

## Métricas relacionadas con la matriz de confusión

1. **Precisión (Precision):**  
   Proporción de predicciones positivas correctas sobre el total de predicciones positivas.

   $$
   \text{Precision} = \frac{TP}{TP + FP}
   $$

   **Importancia:**  
   Alta precisión es crucial cuando el costo de un falso positivo (FP) es alto (por ejemplo, detección de fraudes).

---

2. **Sensibilidad (Recall o Tasa de Verdaderos Positivos):**  
   Proporción de casos positivos detectados correctamente.

   $$
   \text{Recall} = \frac{TP}{TP + FN}
   $$

   **Importancia:**  
   Alta sensibilidad es esencial cuando el costo de un falso negativo (FN) es alto (por ejemplo, detección de enfermedades).

---

3. **F1-Score:**  
   Promedio armónico entre precisión y sensibilidad, útil cuando hay un balance entre ambos.

   $$
   \text{F1-Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
   $$

   **Importancia:**  
   Útil cuando es necesario equilibrar precisión y sensibilidad.

---

4. **Exactitud (Accuracy):**  
   Proporción de predicciones correctas en general.

   $$
   \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
   $$

   **Importancia:**  
   Mide la eficacia global del modelo, pero puede ser engañosa si las clases están desbalanceadas.
  
---  

## Elección de métricas según el problema de negocio
La métrica priorizada depende de las necesidades y riesgos asociados al problema:

- Problemas médicos o de seguridad:
Priorizar la sensibilidad (recall) para minimizar falsos negativos. Ejemplo: Detección de cáncer.

- Sistemas de detección de fraude:
Priorizar la precisión para minimizar falsos positivos, evitando clasificar transacciones legítimas como fraudulentas.

- Sistemas equilibrados:
Usar el F1-Score cuando tanto FP como FN tengan impactos similares, equilibrando precisión y sensibilidad.

- Precisión vs Sensibilidad
Cuando mejoras una métrica, la otra tiende a disminuir debido al cambio en el umbral de decisión del modelo.

**Por eso, es clave entender el contexto del problema para definir el balance adecuado.**

Ejemplo: En un problema médico, es preferible detectar más pacientes enfermos (priorizando la sensibilidad) aunque implique más falsos positivos.
En sistemas de recomendación, se puede priorizar la precisión para evitar enviar recomendaciones irrelevantes.

***La matriz de confusión es, en resumen, una herramienta poderosa para interpretar y adaptar tu modelo de clasificación al impacto del problema en el mundo real.***

# Evaluación de Créditos Bancarios con la Matriz de Confusión

En el contexto de evaluar si una persona es apta para recibir un crédito bancario, la matriz de confusión y las métricas relacionadas pueden ayudarte a tomar decisiones informadas sobre cómo mejorar tu modelo según las prioridades del banco.

## Cómo interpretar los errores en este contexto

### **Falsos Positivos (FP):**
- **Descripción:** El modelo predice que una persona es apta para un crédito, pero en realidad no lo es.
- **Impacto:**
  - El banco otorga créditos a personas que probablemente no podrán pagarlos, lo que genera pérdidas financieras.
  - Este error es especialmente costoso para la institución.
- **Solución:** Se prioriza mejorar la **precisión**, para garantizar que los créditos solo se otorguen a personas verdaderamente aptas.

---

### **Falsos Negativos (FN):**
- **Descripción:** El modelo predice que una persona no es apta, pero en realidad sí lo es.
- **Impacto:**
  - El banco pierde la oportunidad de ganar dinero de clientes que podrían haber pagado sus créditos.
  - Impacta la percepción del cliente hacia la institución financiera.
- **Solución:** Se prioriza mejorar la **sensibilidad (recall)**, para identificar correctamente a más clientes aptos.

---

## Elección de métricas prioritarias

La decisión sobre cuál métrica priorizar depende de las metas del banco y del balance entre riesgos y oportunidades:

- **Si el objetivo es minimizar riesgos financieros:**
  - Prioriza la **precisión**.
  - Garantizas que las personas a las que se apruebe un crédito tengan alta probabilidad de pagar.

- **Si el objetivo es maximizar el número de clientes:**
  - Prioriza la **sensibilidad**.
  - Aceptas un mayor riesgo de incluir personas no aptas, pero captas más clientes que podrían ser buenos pagadores.

- **Si ambos son importantes:**
  - Usa el **F1-score** para encontrar un equilibrio entre precisión y sensibilidad.

---

## Configuración práctica del modelo

Puedes ajustar el umbral de decisión del modelo para inclinarte hacia precisión o sensibilidad según las prioridades del negocio:

- **Mayor umbral:** Reduce los falsos positivos (**mejora la precisión**).
- **Menor umbral:** Reduce los falsos negativos (**mejora la sensibilidad**).

---

## Ejemplo práctico con métricas de la matriz de confusión

Supongamos que tu matriz de confusión para un conjunto de validación es:

|                      | **Apto (Predicción Positiva)** | **No Apto (Predicción Negativa)** |
|----------------------|--------------------------------|------------------------------------|
| **Apto (Real)**      | 80 (TP)                       | 20 (FN)                           |
| **No Apto (Real)**   | 30 (FP)                       | 70 (TN)                           |

### Métricas resultantes:

1. **Precisión (Precision):**  
   Proporción de predicciones positivas correctas sobre el total de predicciones positivas.

   $$
   \text{Precision} = \frac{TP}{TP + FP} = \frac{80}{80 + 30} = 0.73 \, (73\%)
   $$

---

2. **Sensibilidad (Recall):**  
   Proporción de casos positivos correctamente identificados.

   $$
   \text{Recall} = \frac{TP}{TP + FN} = \frac{80}{80 + 20} = 0.80 \, (80\%)
   $$

---

3. **Exactitud (Accuracy):**  
   Proporción de predicciones correctas en general.

   $$
   \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{80 + 70}{80 + 70 + 30 + 20} = 0.75 \, (75\%)
   $$

---

### Interpretación:
- La **precisión** del 73% indica que algunos créditos otorgados fueron a personas no aptas (**FP**).
- La **sensibilidad** del 80% muestra que el modelo detecta la mayoría de los clientes aptos (**TP**), pero pierde algunos (**FN**).
- En este caso:
  - Si el banco prefiere minimizar el riesgo de perder dinero, debería priorizar la **precisión**.
  - Si quiere captar más clientes, priorizaría la **sensibilidad**.

"""

# Crear el gráfico de barras con colores personalizados
plt.figure(figsize=(10, 6))  # Ajustar el tamaño de la figura

# Definir colores para los modelos
colores = ['#FF5733', '#33FF57', '#3357FF', '#FF33A1']  # Lista de colores personalizados

# Crear el gráfico de barras
sns.barplot(x='Modelo', y='Precisión', data=resultados, palette=colores)

# Configurar el diseño del gráfico
plt.title('Exactitud de los Modelos', fontsize=16)
plt.xlabel('Modelos', fontsize=12)
plt.ylabel('Precisión', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotar etiquetas del eje x para mejor legibilidad
plt.tight_layout()  # Ajustar automáticamente el diseño
plt.show()

# Crear el gráfico de barras con colores personalizados
plt.figure(figsize=(10, 6))  # Ajustar el tamaño de la figura

# Definir colores para los modelos
colores = ['#FF5733', '#33FF57', '#3357FF', '#FF33A1']  # Lista de colores personalizados

# Crear el gráfico de barras
sns.barplot(x='Modelo', y='Sensibilidad', data=resultados, palette=colores)

# Configurar el diseño del gráfico
plt.title('Exactitud de los Modelos', fontsize=16)
plt.xlabel('Modelos', fontsize=12)
plt.ylabel('Sensibilidad', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotar etiquetas del eje x para mejor legibilidad
plt.tight_layout()  # Ajustar automáticamente el diseño
plt.show()

# Matriz de confusión utilizando todas las métricas

def construir_modelos():
    global df_banco, resultados

    # Separar variables predictoras y variable objetivo
    X = df_banco.drop('default', axis=1)
    y = df_banco['default']

    # Dividir los datos en conjunto de entrenamiento y prueba
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Escalar las características (opcional, pero útil para ciertos modelos)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Definición de los modelos
    modelos = {
        "Regresión Logística": LogisticRegression(),
        "Árbol de Decisión": DecisionTreeClassifier(),
        "Bosque Aleatorio": RandomForestClassifier(),
        "Naive Bayes": GaussianNB()
    }

    # Lista para almacenar los resultados
    resultados_lista = []

    for nombre, modelo in modelos.items():
        # Entrenar el modelo
        modelo.fit(X_train_scaled, y_train)

        # Realizar predicciones
        y_pred = modelo.predict(X_test_scaled)
        y_prob = modelo.predict_proba(X_test_scaled)[:, 1]  # Probabilidades para la clase positiva

        # Calcular las métricas de evaluación
        exactitud = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        sensibilidad = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_prob)

        # Agregar los resultados a la lista
        resultados_lista.append({
            'Modelo': nombre,
            'Exactitud': exactitud,
            'Precisión': precision,
            'Sensibilidad': sensibilidad,
            'F1-score': f1,
            'AUC': auc
        })

        # Mostrar la matriz de confusión
        cm = confusion_matrix(y_test, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Default", "Default"])
        disp.plot(cmap=plt.cm.Blues)
        plt.title(f'Matriz de Confusión para {nombre}')
        plt.show()

    # Convertir la lista de resultados a un DataFrame
    resultados = pd.DataFrame(resultados_lista)
    print(resultados)

# Llamar a la función para construir los modelos y mostrar resultados
construir_modelos()

# Matriz de confusión usando la métrica presición
# Caso de uso: Si el objetivo del banco es minimizar riesgos financieros:
# Garantizas que las personas a las que se apruebe un crédito tengan alta probabilidad de pagar.

def construir_modelos():
    global df_banco, resultados

    # Separar variables predictoras y variable objetivo
    X = df_banco.drop('default', axis=1)
    y = df_banco['default']

    # Dividir los datos en conjunto de entrenamiento y prueba
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Escalar las características (opcional, pero útil para ciertos modelos)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Definición de los modelos
    modelos = {
        "Regresión Logística": LogisticRegression(),
        "Árbol de Decisión": DecisionTreeClassifier(),
        "Bosque Aleatorio": RandomForestClassifier(),
        "Naive Bayes": GaussianNB()
    }

    # Lista para almacenar los resultados
    resultados_lista = []

    for nombre, modelo in modelos.items():
        # Entrenar el modelo
        modelo.fit(X_train_scaled, y_train)

        # Realizar predicciones
        y_pred = modelo.predict(X_test_scaled)
        y_prob = modelo.predict_proba(X_test_scaled)[:, 1]  # Probabilidades para la clase positiva

        # Calcular las métricas de evaluación
        precision = precision_score(y_test, y_pred)

        # Agregar los resultados a la lista
        resultados_lista.append({
            'Modelo': nombre,
            'Precisión': precision
        })

        # Mostrar la matriz de confusión
        cm = confusion_matrix(y_test, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Default", "Default"])
        disp.plot(cmap=plt.cm.Blues)
        plt.title(f'Matriz de Confusión para {nombre}')
        plt.show()

    # Convertir la lista de resultados a un DataFrame
    resultados = pd.DataFrame(resultados_lista)
    print(resultados)

# Llamar a la función para construir los modelos y mostrar resultados
construir_modelos()

"""## ¿Cómo interpretarlo?
- Verdaderos Negativos (TN): El valor en la esquina superior izquierda (132) representa la cantidad de casos en los que el modelo predijo correctamente que No Default (sin incumplimiento).

- Falsos Positivos (FP): El valor en la esquina superior derecha (9) muestra cuántos casos el modelo predijo como Default, pero en realidad fueron No Default (incorrectamente clasificados como incumplimiento).

- Falsos Negativos (FN): El valor en la esquina inferior izquierda (34) indica cuántos casos el modelo predijo como No Default, pero en realidad fueron Default (incumplimiento, pero no predicho como tal).

- Verdaderos Positivos (TP): El valor en la esquina inferior derecha (25) representa la cantidad de casos en los que el modelo predijo correctamente que había Default (incumplimiento).

##  **Precisión (Precision):**

La precisión se calcula con la fórmula:

$$
\text{Precisión} = \frac{TP}{TP + FP} = \frac{25}{25 + 9} = \frac{25}{34} = 0.735 \text{ o } 73.5\%
$$

Mide cuántas de las predicciones de Default fueron realmente correctas..
"""

# Matriz de confusión para métrica Sensibilidad
# Casos de uso: Si el objetivo del banco es maximizar el número de clientes:
# Aceptas un mayor riesgo de incluir personas no aptas, pero captas más clientes que podrían ser buenos pagadores.


def construir_modelos():
    global df_banco, resultados

    # Separar variables predictoras y variable objetivo
    X = df_banco.drop('default', axis=1)
    y = df_banco['default']

    # Dividir los datos en conjunto de entrenamiento y prueba
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Escalar las características (opcional, pero útil para ciertos modelos)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Definición de los modelos
    modelos = {
        "Regresión Logística": LogisticRegression(),
        "Árbol de Decisión": DecisionTreeClassifier(),
        "Bosque Aleatorio": RandomForestClassifier(),
        "Naive Bayes": GaussianNB()
    }

    # Lista para almacenar los resultados
    resultados_lista = []

    for nombre, modelo in modelos.items():
        # Entrenar el modelo
        modelo.fit(X_train_scaled, y_train)

        # Realizar predicciones
        y_pred = modelo.predict(X_test_scaled)
        y_prob = modelo.predict_proba(X_test_scaled)[:, 1]  # Probabilidades para la clase positiva

        # Calcular las métricas de evaluación
        sensibilidad = recall_score(y_test, y_pred)

        # Agregar los resultados a la lista
        resultados_lista.append({
            'Modelo': nombre,
            'Sensibilidad': sensibilidad
        })

        # Mostrar la matriz de confusión
        cm = confusion_matrix(y_test, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Default", "Default"])
        disp.plot(cmap=plt.cm.Blues)
        plt.title(f'Matriz de Confusión para {nombre}')
        plt.show()

    # Convertir la lista de resultados a un DataFrame
    resultados = pd.DataFrame(resultados_lista)
    print(resultados)

# Llamar a la función para construir los modelos y mostrar resultados
construir_modelos()

"""## ¿Cómo interpretarlo?
- Verdaderos Negativos (TN): El valor en la esquina superior izquierda (132) representa la cantidad de casos en los que el modelo predijo correctamente que No Default (sin incumplimiento).

- Falsos Positivos (FP): El valor en la esquina superior derecha (9) muestra cuántos casos el modelo predijo como Default, pero en realidad fueron No Default (incorrectamente clasificados como incumplimiento).

- Falsos Negativos (FN): El valor en la esquina inferior izquierda (34) indica cuántos casos el modelo predijo como No Default, pero en realidad fueron Default (incumplimiento, pero no predicho como tal).

- Verdaderos Positivos (TP): El valor en la esquina inferior derecha (25) representa la cantidad de casos en los que el modelo predijo correctamente que había Default (incumplimiento).

##  **Sensibilidad (Recall) o Tasa de verdaderos positivos:**

La sensibilidad se calcula con la fórmula:

$$
\text{Sensibilidad} = \frac{TP}{TP + FN} = \frac{25}{25 + 34} = \frac{25}{59} = 0.424 \text{ o } 42.4\%
$$

Mide la capacidad del modelo para identificar correctamente los casos de Default.


"""

# Matriz de confusión usando métrica F1-score
# Casos de uso: si quieres buscar un equilibrio entre minimizar el riesgo financiero
# y maximizar el numero de clientes puedes utilizar la métrica F1-score para encontrar un equilibrio entre las dos

def construir_modelos():
    global df_banco, resultados

    # Separar variables predictoras y variable objetivo
    X = df_banco.drop('default', axis=1)
    y = df_banco['default']

    # Dividir los datos en conjunto de entrenamiento y prueba
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Escalar las características (opcional, pero útil para ciertos modelos)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Definición de los modelos
    modelos = {
        "Regresión Logística": LogisticRegression(),
        "Árbol de Decisión": DecisionTreeClassifier(),
        "Bosque Aleatorio": RandomForestClassifier(),
        "Naive Bayes": GaussianNB()
    }

    # Lista para almacenar los resultados
    resultados_lista = []

    for nombre, modelo in modelos.items():
        # Entrenar el modelo
        modelo.fit(X_train_scaled, y_train)

        # Realizar predicciones
        y_pred = modelo.predict(X_test_scaled)
        y_prob = modelo.predict_proba(X_test_scaled)[:, 1]  # Probabilidades para la clase positiva

        # Calcular las métricas de evaluación
        f1 = f1_score(y_test, y_pred)

        # Agregar los resultados a la lista
        resultados_lista.append({
            'Modelo': nombre,
            'F1-score': f1
        })

        # Mostrar la matriz de confusión
        cm = confusion_matrix(y_test, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Default", "Default"])
        disp.plot(cmap=plt.cm.Blues)
        plt.title(f'Matriz de Confusión para {nombre}')
        plt.show()

    # Convertir la lista de resultados a un DataFrame
    resultados = pd.DataFrame(resultados_lista)
    print(resultados)

# Llamar a la función para construir los modelos y mostrar resultados
construir_modelos()

"""## ¿Cómo interpretarlo?

- **Verdaderos Negativos (TN):**  
El valor en la esquina superior izquierda (**132**) representa la cantidad de casos en los que el modelo predijo correctamente que **No Default** (sin incumplimiento).

- **Falsos Positivos (FP):**  
El valor en la esquina superior derecha (**9**) muestra cuántos casos el modelo predijo como **Default**, pero en realidad fueron **No Default** (incorrectamente clasificados como incumplimiento).

- **Falsos Negativos (FN):**  
El valor en la esquina inferior izquierda (**34**) indica cuántos casos el modelo predijo como **No Default**, pero en realidad fueron **Default** (incumplimiento, pero no predicho como tal).

- **Verdaderos Positivos (TP):**  
El valor en la esquina inferior derecha (**25**) representa la cantidad de casos en los que el modelo predijo correctamente que había **Default** (incumplimiento).

---

Usa el **F1-score** para encontrar un equilibrio entre **precisión** y **sensibilidad**.  

El **F1-Score** es la media armónica entre precisión y sensibilidad. Este valor es útil cuando hay un desequilibrio en las clases (por ejemplo, mucho más **No Default** que **Default**).

La fórmula para calcularlo es:

$$
F1 = 2 \cdot \frac{\text{Precisión} \cdot \text{Sensibilidad}}{\text{Precisión} + \text{Sensibilidad}}
$$

Sustituyendo los valores:

$$
F1 = 2 \cdot \frac{0.735 \cdot 0.424}{0.735 + 0.424} \approx 0.534
$$

## Desafio 2 - Clase 2
El objetivo de este desafio es trabajar con un conjunto de datos donde la variable objetivo o Target (default), que indica si un cliente es buen pagador (0) o mal pagador (1), está desbalanceada. Este desbalance significa que hay una cantidad significativamente mayor de registros de buenos pagadores en comparación con malos pagadores, lo cual afecta negativamente el rendimiento y la evaluación de los modelos de Machine Learning.
Para ello podemos utilizar la técnica de balanceo llamada Oversampling

### 1. Cargar y analizar los datos
Primero, se debe cargar el conjunto de datos y verificar el desbalance en la variable objetivo (default).
### 2. Aplicar técnicas de oversampling
Utilizaremos el método SMOTE (Synthetic Minority Oversampling Technique) para generar ejemplos sintéticos de la clase minoritaria (default = 1).
"""

# Antes de balancear
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Verificar el balance de clases
print("Distribución de clases antes del balanceo:")
print(df_banco['default'].value_counts())

# Separar las características (X) y la variable objetivo (y)
X = df_banco.drop(columns=['default'])
y = df_banco['default']

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Crear y entrenar el modelo
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# Predecir sobre el conjunto de prueba
y_pred = model.predict(X_test)

# Verificar la distribución de clases en el conjunto de entrenamiento antes de SMOTE
print("\nDistribución de clases en el conjunto de entrenamiento antes del balanceo:")
print(Counter(y_train))

# Evaluar el modelo
print("\nReporte de clasificación en el conjunto de prueba:")
print(classification_report(y_test, y_pred))

# Ver la matriz de confusión
print("\nMatriz de confusión:")
print(confusion_matrix(y_test, y_pred))

# Ver la exactitud
print("\nExactitud del modelo:")
print(accuracy_score(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from collections import Counter
from sklearn.preprocessing import OneHotEncoder
from imblearn.over_sampling import SMOTE


# Verificar el balance de clases
print("Distribución de clases antes del balanceo:")
print(df_banco['default'].value_counts())

# Separar las características (X) y la variable objetivo (y)
X = df_banco.drop(columns=['default'])
y = df_banco['default']

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Verificar si hay columnas categóricas en X_train
print("Tipos de datos en las columnas de X_train:")
print(X_train.dtypes)

# Codificar variables categóricas con One-Hot Encoding solo en X_train
X_train_encoded = pd.get_dummies(X_train, drop_first=True)

# Verificar que el tamaño de X_train_encoded y y_train sea consistente
print("\nTamaño de X_train_encoded:", X_train_encoded.shape)
print("Tamaño de y_train:", y_train.shape)

# Verificar la distribución de clases en el conjunto de entrenamiento antes de SMOTE
print("\nDistribución de clases en el conjunto de entrenamiento antes del balanceo:")
print(Counter(y_train))

# Aplicar SMOTE para balancear las clases en el conjunto de entrenamiento
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# Verificar la distribución de clases después de SMOTE
print("\nDistribución de clases en el conjunto de entrenamiento después del balanceo:")
print(Counter(y_train_resampled))

"""### 3 - Entrenar un modelo de Machine Learning
Ahora que tienes los datos balanceados en X_train_resampled y y_train_resampled, puedes proceder a entrenar un modelo de clasificación. Vamos a usar un modelo común como Logistic Regression, pero puedes experimentar con otros modelos (como Random Forest, SVM, XGBoost, etc.).
"""

# Despues de balancear
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Crear y entrenar el modelo
model = LogisticRegression(random_state=42)
model.fit(X_train_resampled, y_train_resampled)

# Predecir sobre el conjunto de prueba
y_pred = model.predict(X_test)

# Evaluar el modelo
print("\nReporte de clasificación en el conjunto de prueba:")
print(classification_report(y_test, y_pred))

# Ver la matriz de confusión
print("\nMatriz de confusión:")
print(confusion_matrix(y_test, y_pred))

# Ver la exactitud
print("\nExactitud del modelo:")
print(accuracy_score(y_test, y_pred))

"""# Análisis de Resultados Antes y Después de Aplicar SMOTE para Balancear las Clases

## Distribución de Clases Antes del Balanceo

- **Clase 0 (sin incumplimiento)**: 700 muestras
- **Clase 1 (con incumplimiento)**: 300 muestras

El conjunto de datos original está desbalanceado, con más instancias de la clase 0 que de la clase 1.

## Distribución de Clases en el Conjunto de Entrenamiento Antes del Balanceo

- **Clase 0**: 490 muestras
- **Clase 1**: 210 muestras

Este desbalance en el conjunto de entrenamiento refleja una mayor representación de la clase 0 en comparación con la clase 1.

## Métricas Antes del Balanceo

### Reporte de Clasificación

| Clase | Precisión | Recall | F1-score | Support |
|-------|-----------|--------|----------|---------|
| 0     | 0.80      | 0.86   | 0.83     | 210     |
| 1     | 0.61      | 0.51   | 0.55     | 90      |
| **Exactitud** |   |   | 0.75     | 300     |
| **Macro avg** | 0.70 | 0.68 | 0.69 | 300 |
| **Weighted avg** | 0.74 | 0.75 | 0.75 | 300 |

### Matriz de Confusión

- **Verdaderos negativos** (clase 0 correctamente predicha): 180
- **Falsos positivos** (clase 0 incorrectamente predicha como clase 1): 30
- **Falsos negativos** (clase 1 incorrectamente predicha como clase 0): 44
- **Verdaderos positivos** (clase 1 correctamente predicha): 46

### Conclusión Antes del Balanceo

- El modelo tiene un buen rendimiento en la **clase 0** (mayor recall), pero no tiene un buen rendimiento en la **clase 1**, con un recall y f1-score bajos.
- La **exactitud** total es del 75%, pero este valor no refleja el desbalance entre las clases, ya que la clase 0 tiene más representación.

## Distribución de Clases Después del Balanceo con SMOTE

- **Clase 0**: 490 muestras
- **Clase 1**: 490 muestras

Al aplicar SMOTE, las clases están ahora balanceadas, con el mismo número de muestras para cada clase.

## Métricas Después del Balanceo con SMOTE

### Reporte de Clasificación

| Clase | Precisión | Recall | F1-score | Support |
|-------|-----------|--------|----------|---------|
| 0     | 0.84      | 0.75   | 0.79     | 210     |
| 1     | 0.54      | 0.67   | 0.59     | 90      |
| **Exactitud** |   |   | 0.73     | 300     |
| **Macro avg** | 0.69 | 0.71 | 0.69 | 300 |
| **Weighted avg** | 0.75 | 0.73 | 0.73 | 300 |

### Matriz de Confusión


- **Verdaderos negativos** (clase 0 correctamente predicha): 158
- **Falsos positivos** (clase 0 incorrectamente predicha como clase 1): 52
- **Falsos negativos** (clase 1 incorrectamente predicha como clase 0): 30
- **Verdaderos positivos** (clase 1 correctamente predicha): 60

### Conclusión Después del Balanceo

- **Clase 0**: La precisión mejora al 84%, pero el recall disminuye al 75%. Esto indica que, aunque el modelo es más preciso, ahora está cometiendo más falsos positivos para la clase 0.
- **Clase 1**: El modelo mejora en recall (67%) pero sigue teniendo una precisión relativamente baja (54%). Esto indica que el modelo predice más correctamente la clase 1, pero aún sigue cometiendo muchos falsos positivos para la clase 1.

## Análisis Comparativo

### Precisión de la Clase 0

- **Antes**: 80%
- **Después**: 84%

La precisión para la clase 0 ha mejorado después de aplicar SMOTE. Sin embargo, esto también ha aumentado el riesgo de falsos positivos.

### Recall de la Clase 1

- **Antes**: 51%
- **Después**: 67%

El recall para la clase 1 ha mejorado significativamente, lo que indica que el modelo ahora es más capaz de identificar la clase minoritaria.

### Exactitud Total

- **Antes**: 75%
- **Después**: 73%

Aunque la exactitud ha disminuido ligeramente, este cambio es esperado. Al balancear las clases, el modelo también aprende a predecir mejor la clase minoritaria, lo que puede reducir la precisión general.

### F1-score

- **Clase 0**: Disminuyó de 0.83 a 0.79
- **Clase 1**: Aumentó de 0.55 a 0.59

El F1-score para la clase 0 ha disminuido, mientras que para la clase 1 ha aumentado, lo que sugiere que el balanceo ha hecho un compromiso entre ambas clases, con mejoras en la clase minoritaria.

## Conclusión General

El balanceo con SMOTE ha tenido un impacto positivo en la capacidad del modelo para predecir la clase 1, mejorando significativamente su recall. Sin embargo, esto ha venido a costa de una disminución en la exactitud total y en el recall de la clase 0. Este es un caso típico cuando se balancean clases desbalanceadas: el modelo mejora su capacidad de detección de la clase minoritaria a expensas de una ligera disminución en el rendimiento general.

## Desafio 3 - Clase 2
Volver a ejecutar los modelos los cuatro modelos que probaron aquí los profes ahora con menos variables vayan a la a la matriz de correlación que que tenían de desafío y que aquí el profe Alejo la construyó en
la primera actividad en esta clase seleccionen aquellas variables que tienen mayores correlaciones entre ellas y ejecuten nuevamente el modelo utilizando solo algunas variables. Comprobar si de esa forma la curacidad de las métricas con las cuales están evaluando el desempeño de su modelo de los modelos tienden a mejorar O tienden a empeorar cuando hacen estos ajustes
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# Selección de las variables relevantes basadas en el mapa de correlación
variables_relevantes = ['credit_history', 'savings', 'installment_as_income_perc', 'property']

# Dividir los datos en conjunto de entrenamiento y prueba
X = df_banco[variables_relevantes]  # Variables seleccionadas
y = df_banco['default']  # Variable objetivo
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenamiento del modelo RandomForest
modelo_rf = RandomForestClassifier()
modelo_rf.fit(X_train, y_train)

# Predicciones
y_pred = modelo_rf.predict(X_test)

# Evaluar el modelo
print("Precisión:", accuracy_score(y_test, y_pred))
print("F1-Score:", f1_score(y_test, y_pred))